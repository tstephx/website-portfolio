<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noindex, nofollow">
    <meta name="description" content="Case study: Analyzing 198 interview records to prove a structural flaw in Amazon's scoring formula, then designing a complete replacement evaluation system â€” hybrid formula, 5-category rubric, 35-question bank, and 5 production artifacts.">
    <title>Pinnacle BPR Scoring Methodology Redesign | Taylor Stephens</title>
    <link rel="icon" href="../../favicon.svg" type="image/svg+xml">
    <meta property="og:title" content="Pinnacle BPR Scoring Methodology Redesign | Taylor Stephens">
    <meta property="og:description" content="Analyzed 198 interview records to prove a structural scoring flaw, then designed and shipped 5 production artifacts replacing every component of the evaluation system.">
    <meta property="og:type" content="article">
    <link rel="canonical" href="https://taylorstephens.io/work/bpr-scoring-pinnacle/">
    <meta name="theme-color" content="#1a3a6b">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:wght@600;700&family=DM+Mono:wght@400;500&family=Epilogue:wght@400;500;600;700&family=Newsreader:ital,opsz,wght@0,6..72,400;0,6..72,600;1,6..72,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../css/styles.css">
    <link rel="stylesheet" href="../../css/case-study.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.7/dist/chart.umd.min.js" defer></script>
    <script src="../../js/progress-bar.js" defer></script>
</head>
<body>

    <div class="reading-progress"></div>
    <a class="skip-link" href="#main-content">Skip to content</a>

    <nav class="back-nav" aria-label="Back navigation">
        <a href="../../index.html#projects">&larr; Back to Portfolio</a>
    </nav>

    <main id="main-content" class="case-study">

        <header class="cs-header">
            <h1>Finding the Mathematical Flaw No One Else Saw</h1>
            <p class="cs-tagline">I reverse-engineered 198 interview records to prove why Amazon&rsquo;s scoring formula was rejecting qualified candidates &mdash; then designed and shipped the complete replacement: hybrid formula, 5-category rubric, 35-question scoring template, and 5 production artifacts.</p>
            <div class="tech-pills">
                <span class="tech-pill">Root Cause Analysis</span>
                <span class="tech-pill">Measurement System Design</span>
                <span class="tech-pill">Behaviorally Anchored Rubrics</span>
                <span class="tech-pill">Calibration Protocols</span>
            </div>
        </header>

        <div class="scope-callout">
            <div class="scope-item">
                <span class="scope-label">Timeline</span>
                <span class="scope-value">2024&ndash;2025</span>
            </div>
            <div class="scope-item">
                <span class="scope-label">My Role</span>
                <span class="scope-value">Design Strategist &amp; Lead Analyst</span>
            </div>
            <div class="scope-item">
                <span class="scope-label">Data Analyzed</span>
                <span class="scope-value">198 interviews, 36 panelists, individual scores</span>
            </div>
            <div class="scope-item">
                <span class="scope-label">Delivered</span>
                <span class="scope-value">5 production artifacts, 3 formula iterations</span>
            </div>
        </div>

        <section class="cs-section" id="problem">
            <h2>Approval Rates Collapsed &mdash; Nobody Knew Why</h2>
            <p>Amazon&rsquo;s delivery network expands by promoting top-performing Delivery Service Partners (independent business owners operating delivery fleets) into &ldquo;Pinnacle&rdquo; assignments &mdash; premium stations with higher volume and revenue. Before a partner can expand, they must pass a panel interview called the Business Plan Review (BPR), where three interviewers score them across leadership and operational criteria.</p>
            <p>This gateway interview was failing. Approval rates collapsed from 85.7% to 56.0% &mdash; a 29.7-percentage-point drop &mdash; and no one could explain why. Qualified candidates were being rejected at unprecedented rates, blocking a critical growth pipeline worth $8.3M in revenue protection.</p>

            <div class="chart-container">
                <div class="chart-label">Approval Rate Collapse</div>
                <canvas id="approvalChart" aria-label="Bar chart showing approval rate decline from 85.7% to 56% and target recovery to 65-70%"></canvas>
            </div>

            <p>Leadership assumed the problem was calibration &mdash; panelists weren&rsquo;t scoring consistently. The scoring data confirmed the inconsistency but pointed somewhere else entirely. For DSP short code FOTL, one panelist scored <strong>5</strong> while another scored <strong>57</strong> &mdash; on the same interview. For KRPT, scores ranged from <strong>10 to 59</strong> across three panelists. Across the full dataset, 77% of panels showed 30+ point scoring gaps on the same candidate.</p>

            <div class="chart-container">
                <div class="chart-label">Worst-Case Scoring Gaps &mdash; Same Candidate, Same Interview</div>
                <canvas id="scoringGapChart" aria-label="Horizontal bar chart showing panelist scoring gaps: FOTL 5 vs 57, KRPT 10 vs 59, BJVL 28 vs 73, DNPL 35 vs 71, MVXT 42 vs 78"></canvas>
            </div>

            <p>But the variance wasn&rsquo;t the root cause. It was the formula that turned that variance into bad decisions.</p>

            <div class="insight-callout">
                Everyone was looking at the people. I looked at the math. The scoring formula had a structural flaw that no amount of panelist training could fix &mdash; a single dissenter could mathematically override two supporters.
            </div>
        </section>

        <section class="cs-section" id="approach">
            <h2>Forensic Analysis of 198 Interviews</h2>
            <p>I approached this as a forensic analysis. Before proposing solutions, I needed to understand exactly why the system was producing bad outcomes. That meant reverse-engineering the scoring formula from historical data &mdash; not just running summary statistics, but reconstructing the exact mathematical relationship between individual panelist scores and final decisions.</p>

            <h3>Three-Phase Investigation</h3>
            <ul>
                <li><strong>Data extraction:</strong> Pull 198 interview records with individual panelist scores across 36 reviewers, final outcomes, and candidate metadata. Build an interactive analysis dashboard to surface patterns invisible in spreadsheets.</li>
                <li><strong>Formula reconstruction:</strong> Reverse-engineer the exact formula by analyzing how individual scores mapped to final decisions. Map every edge case where outcomes diverged from panel consensus &mdash; the cases where 2-1 support still produced rejection.</li>
                <li><strong>Root cause identification:</strong> Isolate three distinct failure modes: (1) scoring inconsistency from subjective &ldquo;Raises/Meets/Lowers the Bar&rdquo; language with no behavioral anchoring, (2) majority votes overridden by rigid point thresholds, and (3) no correlation between scoring patterns and post-launch DSP performance.</li>
            </ul>

            <p>The analysis made the case for wholesale replacement, not incremental adjustment. Every component was broken: the rating scale, the categories, the formula mechanics, the debrief protocol, and the question framework. I needed to redesign all of it.</p>
        </section>

        <section class="cs-section" id="technical">
            <h2>I Replaced Every Component of the System</h2>

            <h3>The Single-Dissent Veto</h3>
            <p>The inherited formula used mean averaging with rigid point thresholds. This created a hidden property: a single low score could drag the mean below the cutoff even when two panelists scored favorably. A 2-1 panel in favor of a candidate could still produce a rejection. I proved this wasn&rsquo;t theoretical &mdash; it was happening in the data.</p>

            <div class="vote-comparison">
                <div class="vote-scenario scenario-old">
                    <h4>Old Formula (Mean)</h4>
                    <div class="vote-icons">
                        <div class="vote-icon vote-yes">+</div>
                        <div class="vote-icon vote-yes">+</div>
                        <div class="vote-icon vote-no">&minus;</div>
                    </div>
                    <div class="vote-result result-rejected">Rejected</div>
                    <p class="vote-caption">One low score drags mean below threshold</p>
                </div>
                <div class="vote-scenario scenario-new">
                    <h4>New Formula (Median)</h4>
                    <div class="vote-icons">
                        <div class="vote-icon vote-yes">+</div>
                        <div class="vote-icon vote-yes">+</div>
                        <div class="vote-icon vote-no">&minus;</div>
                    </div>
                    <div class="vote-result result-approved">Approved</div>
                    <p class="vote-caption">Median resists outliers &mdash; middle score wins</p>
                </div>
            </div>

            <h3>The Hybrid Scoring Formula</h3>
            <p>I designed a hybrid system anchored in median-based scoring &mdash; naturally resistant to outliers. The formula evolved through 3 major iterations, each driven by a specific failure identified in testing.</p>

            <div class="formula-box">
                <div class="formula-label">Final Scoring Formula (v3)</div>
                <div class="formula">
                    Final Score = <span class="accent">80%</span> Median + <span class="accent">15%</span> Vote Weight + <span class="accent">5%</span> Deliverables
                </div>
                <p class="formula-note">At 80% median weight, rubric scores effectively determine the decision &mdash; soft factors can shift a borderline candidate but cannot override evidence</p>
            </div>

            <ul>
                <li><strong>80% Median Score:</strong> Each panelist independently scores 5 weighted categories on a 0&ndash;100 scale. The median of 3 panelist totals becomes the core score. A single outlier &mdash; like the panelist who scored FOTL at 5 when another scored 57 &mdash; cannot distort the median.</li>
                <li><strong>15% Vote Weight:</strong> Binary recommendation from each panelist. All 3 recommend = 100% (15 pts); 2 recommend = 66.6% (10 pts); 1 recommends = 33.3% (5 pts). This bridges granular rubric scoring with holistic judgment.</li>
                <li><strong>5% Deliverables Quality:</strong> Pre-interview assessment of Pro Forma P&amp;L (50 pts) and Business Plan (50 pts). Candidates who submit thin deliverables consistently underperform in interviews &mdash; this gate catches them before panel time is invested.</li>
            </ul>

            <h3>Formula Evolution</h3>
            <p>The formula wasn&rsquo;t designed in one pass. Each iteration fixed a specific failure mode uncovered through testing and stakeholder feedback.</p>

            <table class="data-table">
                <caption>Three formula versions, each driven by a discovered failure</caption>
                <thead>
                    <tr>
                        <th>Version</th>
                        <th>Formula</th>
                        <th>What Changed &amp; Why</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="highlight">v1</td>
                        <td>60/25/15 (Consistency Bonus)</td>
                        <td>Introduced median to prevent veto. But the Consistency Bonus (15 &times; max(0, 1 &minus; SD/12)) penalized legitimate disagreement &mdash; if one panelist correctly identified a gap, the SD penalty could flip the decision.</td>
                    </tr>
                    <tr>
                        <td class="highlight">v2</td>
                        <td>60/25/15 (Deliverables)</td>
                        <td>Replaced Consistency Bonus with Document Quality. Stopped punishing disagreement. But 60% median weight still left room for soft factors to override rubric evidence.</td>
                    </tr>
                    <tr>
                        <td class="highlight">v3 (final)</td>
                        <td>80/15/5</td>
                        <td>Elevated median from 60% to 80%. At this weight, rubric scores determine the outcome. Vote weight and deliverables can shift borderline cases but cannot override strong evidence.</td>
                    </tr>
                </tbody>
            </table>

            <h3>5-Category Weighted Framework</h3>
            <p>The inherited system used 5 loosely defined categories with a subjective &ldquo;Raises/Meets/Lowers the Bar&rdquo; scale. I replaced it with 5 business-critical dimensions weighted by their predictive importance for Pinnacle success, each scored on a 0&ndash;100 behaviorally anchored scale.</p>

            <table class="data-table">
                <caption>Category weights reflect which dimensions most predict expansion success</caption>
                <thead>
                    <tr>
                        <th>Category</th>
                        <th>Weight</th>
                        <th>Why This Weight</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="highlight">Financial Management</td>
                        <td>25%</td>
                        <td>Under-capitalized DSPs most likely to degrade post-launch</td>
                    </tr>
                    <tr>
                        <td class="highlight">Risk Management</td>
                        <td>25%</td>
                        <td>DSPs without risk protocols fail fastest at dual-site scale</td>
                    </tr>
                    <tr>
                        <td>Execution Strategy</td>
                        <td>20%</td>
                        <td>Safety continuity and fleet scaling at doubled operations</td>
                    </tr>
                    <tr>
                        <td>Hiring &amp; Seasonality</td>
                        <td>20%</td>
                        <td>Labor pool adaptation, peak season ramp, retention</td>
                    </tr>
                    <tr>
                        <td>Culture &amp; Communication</td>
                        <td>10%</td>
                        <td>Important but harder to score objectively; less directly predictive</td>
                    </tr>
                </tbody>
            </table>

            <p>Each category replaced vague &ldquo;Meets the Bar&rdquo; language with a 5-band behaviorally anchored scale. For Financial Management, a &ldquo;Strength&rdquo; score (90&ndash;100) requires cash reserves exceeding $100K, month-by-month cash flow projections, multiple financing sources, and proven controls. &ldquo;Mild Concerns&rdquo; (60&ndash;74) means general statements about cost management with no detail on financial controls. The specificity removes ambiguity &mdash; panelists evaluate the same observable behaviors instead of personal interpretation.</p>

            <h3>4-Tier Decision Architecture</h3>
            <p>The old system had a single binary threshold: pass or fail. A candidate scoring 74 received the same outcome as one scoring 49. I designed a 4-tier system that creates space for nuance.</p>

            <table class="data-table">
                <caption>Decision tiers replaced the prior binary pass/fail</caption>
                <thead>
                    <tr>
                        <th>Score</th>
                        <th>Decision</th>
                        <th>Action</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="highlight">&ge;80%</td>
                        <td>Incline</td>
                        <td>Approval authority process begins</td>
                    </tr>
                    <tr>
                        <td class="highlight">75&ndash;79%</td>
                        <td>Verify / Audit</td>
                        <td>Additional rubric and feedback review</td>
                    </tr>
                    <tr>
                        <td><75%</td>
                        <td>Non-incline</td>
                        <td>Pinnacle process ends</td>
                    </tr>
                    <tr>
                        <td><45%</td>
                        <td>Unprepared</td>
                        <td>Triggers review of upstream selection process</td>
                    </tr>
                </tbody>
            </table>

            <p>An additional override: any single category scoring below 30 triggers automatic review regardless of total score. A strong aggregate cannot mask a catastrophic gap in one dimension.</p>

            <h3>Calibration Protocol</h3>
            <p>The old system had an unstructured post-interview debrief where the first panelist to speak anchored everyone else&rsquo;s judgment. I designed a 5-step protocol that enforces independent assessment before discussion.</p>

            <div class="process-timeline">
                <div class="timeline-item completed">
                    <div class="timeline-week">Step 1</div>
                    <div class="timeline-title">Independent Scoring</div>
                    <div class="timeline-desc">Panelists finalize category scores and evidence notes before any discussion &mdash; preventing anchoring bias.</div>
                </div>
                <div class="timeline-item completed">
                    <div class="timeline-week">Step 2</div>
                    <div class="timeline-title">Variance Detection</div>
                    <div class="timeline-desc">System flags any category where panelist spread exceeds 20 points. Flagged categories must be discussed.</div>
                </div>
                <div class="timeline-item completed">
                    <div class="timeline-week">Step 3</div>
                    <div class="timeline-title">Evidence-Based Calibration</div>
                    <div class="timeline-desc">For flagged categories, each panelist cites specific, verifiable evidence. Discussion centers on what was observed, not what was felt.</div>
                </div>
                <div class="timeline-item completed">
                    <div class="timeline-week">Step 4</div>
                    <div class="timeline-title">Consensus or Documented Dissent</div>
                    <div class="timeline-desc">Panelists may update scores after calibration, or record dissent with rationale. Disagreement is legitimate when evidence-backed.</div>
                </div>
                <div class="timeline-item completed">
                    <div class="timeline-week">Step 5</div>
                    <div class="timeline-title">Red Flag Override</div>
                    <div class="timeline-desc">Any category below 30, unresolved BOCs, or NSF history can override favorable numeric scores. The system doesn&rsquo;t let aggregate strength mask critical gaps.</div>
                </div>
            </div>

            <h3>What I Actually Shipped</h3>
            <p>This wasn&rsquo;t a methodology paper. I delivered 5 production artifacts that function as an integrated system &mdash; each standalone-referenceable, but designed to reinforce the others.</p>
            <ul>
                <li><strong>Production Scoring Template:</strong> 35 evaluation questions across 5 categories, each with a 5-band behaviorally anchored rubric on a 0&ndash;100 scale. The working document panelists use during every BPR.</li>
                <li><strong>18-Page Rubric Development Specification:</strong> The architectural blueprint &mdash; problem definition with quantification, VTRA success benchmark (the 139/140 gold-standard candidate whose behaviors defined each rubric band), 3-tier question bank, pilot program design, and digital tool requirements.</li>
                <li><strong>Panelist Guide (4 pages):</strong> Step-by-step execution &mdash; 40-minute prep workflow, 60-minute interview timeline, Green/Yellow/Red decision protocol, and structured 15-minute debrief.</li>
                <li><strong>Interview Playbook &amp; Training Guide (7 pages):</strong> Rubric definitions, 60+ questions with 3 diagnostic lenses (Systematic Approach, Multiplicative Complexity, Hidden Dependencies), bias mitigation training, and red/yellow flag system.</li>
                <li><strong>Interactive Analysis Dashboard:</strong> Processes all 198 interview records. Surfaces scoring patterns, panelist calibration tendencies across 36 reviewers, year-over-year trends, and close-call analysis. The diagnostic engine that made the dysfunction visible.</li>
            </ul>
        </section>

        <section class="cs-section" id="results">
            <h2>Veto Eliminated, Pipeline Unblocked</h2>

            <div class="metrics-row">
                <div class="metric-card">
                    <span class="metric-number">0</span>
                    <span class="metric-label">Veto Power</span>
                    <span class="metric-subtitle">Single-dissent override eliminated</span>
                </div>
                <div class="metric-card">
                    <span class="metric-number">30+ &rarr; &lt;15</span>
                    <span class="metric-label">Scoring Gap (Points)</span>
                    <span class="metric-subtitle">Target: halve inter-rater variance</span>
                </div>
                <div class="metric-card">
                    <span class="metric-number">65&ndash;70%</span>
                    <span class="metric-label">Target Approval Rate</span>
                    <span class="metric-subtitle">Restored from 56%</span>
                </div>
                <div class="metric-card">
                    <span class="metric-number">5</span>
                    <span class="metric-label">Production Artifacts</span>
                    <span class="metric-subtitle">Complete replacement system</span>
                </div>
            </div>

            <div class="chart-container">
                <div class="chart-label">Scoring Variance: Before vs. Target</div>
                <canvas id="varianceChart" aria-label="Horizontal bar chart showing panels with 30+ point scoring gaps at 77% before redesign, target under 15-point gaps after"></canvas>
            </div>

            <h3>System Capability: Before vs. After</h3>
            <p>Eight dimensions of evaluation quality, scored 0&ndash;100. The legacy system scored below 25 on every dimension.</p>

            <div class="bar-comparison">
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Evidence Requirements</span>
                        <span class="bar-value">10 &rarr; 90</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 90%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Panelist Guidance</span>
                        <span class="bar-value">20 &rarr; 90</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 90%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Decision Defensibility</span>
                        <span class="bar-value">15 &rarr; 88</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 88%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Scoring Precision</span>
                        <span class="bar-value">20 &rarr; 85</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 85%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Calibration Protocol</span>
                        <span class="bar-value">5 &rarr; 85</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 85%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Question Depth</span>
                        <span class="bar-value">25 &rarr; 85</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 85%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Inter-rater Reliability</span>
                        <span class="bar-value">23 &rarr; 80</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 80%;"></div>
                    </div>
                </div>
                <div class="bar-comparison-item">
                    <div class="bar-comparison-label">
                        <span>Predictive Validity</span>
                        <span class="bar-value">12 &rarr; 75</span>
                    </div>
                    <div class="bar-track">
                        <div class="bar-fill" style="width: 75%;"></div>
                    </div>
                </div>
            </div>

            <p>The new methodology eliminated the single-dissent veto problem entirely. At 80% median weight, no individual panelist can override the other two. The behaviorally anchored rubrics target a reduction from 30+ point scoring gaps (77% of panels) to under 15-point average variance &mdash; more than halving the inconsistency that made decisions indefensible.</p>

            <p>The approval pathway was restored to a 65&ndash;70% target rate, unblocking a Pinnacle expansion pipeline that had stalled for months. The 4-tier decision architecture added the Verify/Audit zone (75&ndash;79%) for borderline candidates, replacing a binary pass/fail that treated a score of 74 the same as 49.</p>

            <p>But the real shift was structural. Every decision is now traceable: rubric scores tied to behavioral anchors, evidence fields requiring 3&ndash;5 bullet points per category, calibration discussion documented, and dissent recorded with rationale. If a decision is challenged, the evidence trail exists.</p>
        </section>

        <section class="cs-section" id="lessons">
            <h2>What This Taught Me</h2>
            <ul class="lessons-list">
                <li>When a system produces bad outcomes, the problem is often structural, not behavioral. Training people harder doesn&rsquo;t fix a broken formula &mdash; you have to look at the math.</li>
                <li>Proving a flaw exists is harder than fixing it. The reverse-engineering work to demonstrate the veto property took longer than designing the replacement. But proof is what creates organizational willingness to change.</li>
                <li>Design decisions should be driven by discovered failure, not theory. The formula evolved through 3 versions because each iteration revealed a new problem &mdash; the Consistency Bonus penalized legitimate disagreement, the 60% median weight left too much room for soft factors. Shipping v3 instead of v1 was the difference between an improvement and a solution.</li>
                <li>A methodology is only as good as its artifacts. The formula, rubric, question bank, and calibration protocol are intellectually interesting &mdash; but what actually changed behavior was the production scoring template panelists hold in their hands, the playbook they train from, and the dashboard that shows them the data.</li>
            </ul>
        </section>

        <section class="cs-cta">
            <p>Next: <a href="../cfa-dsp-application/dsp-application.html">DSP Application Redesign</a></p>
        </section>

    </main>

    <footer>
        <p>&copy; 2026 Taylor Stephens. All rights reserved.</p>
    </footer>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
        var accent = '#1a3a6b';
        var muted = 'rgba(107, 107, 107, 0.3)';
        var red = '#dc3545';
        var green = '#16a34a';
        var defaults = {
            responsive: true,
            maintainAspectRatio: true,
            animation: { duration: 1200, easing: 'easeOutQuart' },
            plugins: {
                legend: { display: false },
                tooltip: {
                    backgroundColor: '#1a1a1a',
                    titleFont: { family: "'Epilogue', sans-serif", size: 12 },
                    bodyFont: { family: "'Epilogue', sans-serif", size: 12 },
                    padding: 10,
                    cornerRadius: 4
                }
            }
        };

        // Approval rate chart
        new Chart(document.getElementById('approvalChart'), {
            type: 'bar',
            data: {
                labels: ['2024 Rate', 'Collapsed (2025)', 'New Target'],
                datasets: [{
                    data: [85.7, 56, 67.5],
                    backgroundColor: [muted, red, green],
                    borderRadius: 4,
                    barThickness: 60
                }]
            },
            options: Object.assign({}, defaults, {
                aspectRatio: 2.5,
                scales: {
                    y: {
                        beginAtZero: true,
                        max: 100,
                        grid: { color: 'rgba(0,0,0,0.04)' },
                        ticks: {
                            font: { family: "'Epilogue', sans-serif", size: 11 },
                            color: '#6b6b6b',
                            callback: function(v) { return v + '%'; }
                        }
                    },
                    x: {
                        grid: { display: false },
                        ticks: {
                            font: { family: "'Epilogue', sans-serif", size: 11, weight: 600 },
                            color: '#3d3d3d'
                        }
                    }
                }
            })
        });

        // Scoring dysfunction: floating bars showing gap between panelist scores
        new Chart(document.getElementById('scoringGapChart'), {
            type: 'bar',
            data: {
                labels: ['FOTL (52-pt gap)', 'KRPT (49-pt gap)', 'BJVL (45-pt gap)', 'DNPL (36-pt gap)', 'MVXT (36-pt gap)'],
                datasets: [{
                    data: [[5, 57], [10, 59], [28, 73], [35, 71], [42, 78]],
                    backgroundColor: 'rgba(220, 53, 69, 0.65)',
                    borderColor: red,
                    borderWidth: 1,
                    borderRadius: 3,
                    barThickness: 20
                }]
            },
            options: Object.assign({}, defaults, {
                indexAxis: 'y',
                aspectRatio: 2,
                scales: {
                    x: {
                        min: 0, max: 100,
                        grid: { color: 'rgba(0,0,0,0.04)' },
                        ticks: {
                            font: { family: "'Epilogue', sans-serif", size: 11 },
                            color: '#6b6b6b'
                        }
                    },
                    y: {
                        grid: { display: false },
                        ticks: {
                            font: { family: "'Epilogue', sans-serif", size: 10, weight: 600 },
                            color: '#3d3d3d'
                        }
                    }
                }
            })
        });

        // Variance chart
        new Chart(document.getElementById('varianceChart'), {
            type: 'bar',
            data: {
                labels: ['Before: Panels with 30+ pt Gaps', 'Target: Under 15-pt Gaps'],
                datasets: [{
                    data: [77, 28],
                    backgroundColor: [red, green],
                    borderRadius: 4,
                    barThickness: 60
                }]
            },
            options: Object.assign({}, defaults, {
                indexAxis: 'y',
                aspectRatio: 2.5,
                scales: {
                    x: {
                        beginAtZero: true,
                        max: 100,
                        grid: { color: 'rgba(0,0,0,0.04)' },
                        ticks: {
                            font: { family: "'Epilogue', sans-serif", size: 11 },
                            color: '#6b6b6b',
                            callback: function(v) { return v + '%'; }
                        }
                    },
                    y: {
                        grid: { display: false },
                        ticks: {
                            font: { family: "'Epilogue', sans-serif", size: 11, weight: 600 },
                            color: '#3d3d3d'
                        }
                    }
                }
            })
        });
    });
    </script>

</body>
</html>
